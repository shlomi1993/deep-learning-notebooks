{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shlomi1993/deep-learning-notebooks/blob/main/3_autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT77AcUlZ-n0"
      },
      "source": [
        "# Deep Learning\n",
        "# Chapter 3 - Autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkVKQ0XRHZ7Y"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dewblBqsHe45"
      },
      "source": [
        "Importing necessary modules. torch is used for all tensor operations. typing is used to define input types more clearly (like tuples or lists), and tqdm is used for progress display when iterating through the loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bvi3F8HcHnLP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from typing import Tuple, List, Dict, Union, Optional\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUJyIhpFHs3u"
      },
      "source": [
        "## Implement `Multinomial` Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4JhXHwSG0Og"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KTZ3tJFGawf"
      },
      "source": [
        "The task is to implement a function `my_sampler` that gets `size`, `dist`, `requires_grad` parameters and samples integers according to a given discrete probability distribution. This function is a custom implementation for `torch.multinomial`.\n",
        "\n",
        "The function should return a PyTorch tensor of the specified shape (`size`) containing integers from `0` to `n-1`, where `n` is the length of the distribution dist.\n",
        "\n",
        "Each integer `i` should be sampled with probability `dist[i]`. The function must validate the input distribution, optionally support gradient tracking, and use only one Python loop, which must begin with `for i in range(len(dist) - 1):`.\n",
        "\n",
        "Using `torch.multinomial` or similar high-level sampling functions is not allowed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myKk785gIDUZ"
      },
      "source": [
        "### Function Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk6egJYhIFVU"
      },
      "source": [
        "The function `my_sampler` accepts three parameters:\n",
        "- `size` - Determines the shape of the output tensor.\n",
        "- `dist` - A list of probabilities defining the discrete distribution\n",
        "- `requires_grad` - A boolean flag indicating whether the returned tensor should track gradients for PyTorch's autograd system.\n",
        "\n",
        "The function returns a PyTorch tensor filled with integers sampled from the range `[0, len(dist) - 1]` according to the probabilities specified in dist. The returned tensor will have the shape specified by size. If `requires_grad=True` is passed, the tensor will track gradients, enabling it to be used in autograd-based computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S9Yj8AQl_iQz"
      },
      "outputs": [],
      "source": [
        "def my_sampler(size: Union[int, Tuple[int, ...]], dist: List[float], requires_grad: bool = False) -> torch.Tensor:\n",
        "    # Convert dist to a torch tensor of type float32\n",
        "    dist_tensor = torch.tensor(dist, dtype=torch.float32)\n",
        "\n",
        "    # Validate the distribution\n",
        "    assert torch.all(dist_tensor > 0), \"All probabilities must be positive\"\n",
        "    assert torch.abs(dist_tensor.sum() - 1.0) < 1e-6, \"Probabilities must sum to 1\"\n",
        "\n",
        "    # Compute cumulative distribution\n",
        "    cumsum = torch.cumsum(dist_tensor, dim=0)\n",
        "\n",
        "    # Sample uniform values\n",
        "    U = torch.rand(size)\n",
        "\n",
        "    # Allocate tensor for samples\n",
        "    samples = torch.empty(size, dtype=torch.float32)\n",
        "\n",
        "    # Loop through the intervals to assign values\n",
        "    for i in tqdm(range(len(dist) - 1), desc=\"Assign values\"):\n",
        "        condition = (U >= cumsum[i]) & (U < cumsum[i + 1])\n",
        "        samples[condition] = float(i + 1)\n",
        "\n",
        "    # Assign value 0 where U < cumsum[0] and value n where U >= cumsum[-1]\n",
        "    samples[U < cumsum[0]] = 0.0\n",
        "    samples[U >= cumsum[-1]] = float(len(dist) - 1)\n",
        "\n",
        "    # Set requires_grad if requested\n",
        "    if requires_grad:\n",
        "        samples.requires_grad_()\n",
        "\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLvkmsvWIvkE"
      },
      "source": [
        "### Implementation Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Wb_nAVGIzxG"
      },
      "source": [
        "1. **Converting and balidating the distribution:** The input list `dist` is first converted to a PyTorch tensor with type `float32`. Two assertions are then made: one to ensure all probabilities are strictly positive, and another to ensure their sum is approximately 1 (allowing a small margin of error due to floating-point arithmetic). These checks ensure that the distribution is valid for sampling.\n",
        "\n",
        "2. **Compute the cumulative distribution:** The cumulative sum of the distribution is computed using the allowed PyTorch function `torch.cumsum`. This is a common method used in inverse transform sampling to define ranges for mapping a uniform random value to one of the discrete classes. For example, with `dist = [0.1, 0.2, 0.7]`, the cumulative distribution would be `[0.1, 0.3, 1.0]`.\n",
        "\n",
        "3. **Sample uniform random values:** Uniform random values in the range `[0, 1)` are sampled using the allowd PyTorch function `torch.rand` with the given `size`. These values are used to decide which class (0, 1, 2, ...) each sample should be assigned to, based on where the uniform value falls in the cumulative distribution.\n",
        "\n",
        "4. **Allocate output tensor:** An empty tensor with the same shape is created using the `torch.empty` constructor that I assumed we can use as we need a simple function for tensor allocation, with data type `float32`. This tensor will be filled with the sampled class values in the following steps.\n",
        "\n",
        "5. **Loop through the cumulative distribution:** A single loop is used to iterate over the distribution intervals, starting from `0` to `len(dist) - 2`, as required by the assignment. For each interval `i`, a condition is checked to see which uniform values fall between `cumsum[i]` and `cumsum[i + 1]`. These positions are then assigned the value `i + 1`, indicating they belong to the corresponding class. This loop fills in all sampled values except for those in the first and last interval, which are handled separately.\n",
        "\n",
        "6. **Handle edge cases:** The values of the tensor where the uniform random value is less than `cumsum[0]` are assigned the class `0`, since they fall into the first bucket. Similarly, values where the uniform is greater than or equal to `cumsum[-1]` are assigned the highest class index, which is `len(dist) - 1`. This ensures all possible uniform values are properly categorized into discrete class indices.\n",
        "\n",
        "7. **Set requires grad if needed:** If the `requires_grad` parameter is set to `True`, the output tensor is updated in-place to track gradients using `requires_grad_()`. This is useful in cases where the samples will be part of a computation graph and used in optimization.\n",
        "\n",
        "8. **Return the sampled tensor:** The completed tensor is returned, containing values sampled according to the specified distribution. Each entry is an integer index from `0` to `len(dist) - 1`, where the frequency of each index matches its corresponding probability in the distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqKhl_Z3Lyzt"
      },
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcZbO8dnL0tT"
      },
      "source": [
        "The following tests validates:\n",
        "- Sanity - by checking the assignment given example\n",
        "- Shapes\n",
        "- Sample values\n",
        "- Usage of requires_grad flag\n",
        "- Appropriate handling of invalid inputs such as negative probabilities or distributions that do not sum to 1.\n",
        "- Edge cases where the uniform value falls outside the first or last intervals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QjDViLKjbSV"
      },
      "source": [
        "###### Test given assignment output example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "548780e29e2d471c86f592a238d0753b",
            "4260716dd8d2405193896cfc6bdc98d1",
            "0979f694ff7b4ddd81e8b8456f97c870",
            "11c5f5eaa4834829bb16277158e2a60a",
            "bc3576f2da29432380413ccb116bce54",
            "11bdb71eb74b49fa8b3df4d86af0da15",
            "d4ce450dda88446faf9532b9dbbc95f9",
            "b5f1cd745a3548cd9a70128971a00ad4",
            "cf937d809dfe4680bf0143b71997310d",
            "b422a434f7384c4bbe1e394e94790d9b",
            "a9c6a20f937a4818b120cd5aaaa17a87",
            "e05fb24ce0234f7993472f6cc0745c52",
            "9cd7b76c2d2840ae92cce081d749dbe4",
            "8234822559564fc1afed5dbfd680547a",
            "02bc012bac824bcd8cf050c17f662dd7",
            "83053453b3a84569983ce8fc18529d60",
            "41fa2c2fc03c4de29b0f12f125d268e7",
            "051666675d60430ca85f81f5df98dacc",
            "087ffb310d1d4d1ead70f5086d062d65",
            "a98a02f9fba248b39f3bd7d552b1ce54",
            "f98dee95bc8c414ca675aaad053cedfd",
            "e313cb117a4e414c8e709ab3a4d0b753"
          ]
        },
        "id": "YYmIdZLDjfgd",
        "outputId": "549e535d-b440-4cbe-876b-74d87edead64"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "548780e29e2d471c86f592a238d0753b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Assign values:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e05fb24ce0234f7993472f6cc0745c52",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Assign values:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Test passed!\n"
          ]
        }
      ],
      "source": [
        "s = my_sampler(10, [0.5, 0.5])\n",
        "assert set(s.tolist()).issubset({0.0, 1.0}), \"Sample output includes unexpected values\"\n",
        "A = my_sampler((2, 8), [0.1, 0.2, 0.7], requires_grad=True)\n",
        "assert A.shape == (2, 8), \"Shape mismatch in example tensor\"\n",
        "assert A.requires_grad, \"Example tensor should require grad\"\n",
        "assert A.grad is None, \"Gradient should be None before backprop\"\n",
        "assert set(torch.unique(A).tolist()).issubset([0.0, 1.0, 2.0]), \"Unexpected values in sampled tensor\"\n",
        "print(\"✅ Test passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhIl0iBMjs6x"
      },
      "source": [
        "###### Test correct shape of output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "b70a896239cd4c33a305abe5ff52de31",
            "cf63aeaed64b4cb496fd901943723d49",
            "5275dd706e16496a8011e0b897be26f8",
            "9b4872057b2b46c1966244f868a0f347",
            "dace081fd0974cb990a808a595ee02ea",
            "8961127b761e423ba3a13cb0785d7701",
            "7f939b7bc67d4f6a98a194b2dc78cde3",
            "b37c08d9055d4861a52d1e91be2f8e74",
            "b70cd3bbcf9f4e17ad5931b8fab26abf",
            "4168e6eea4084b51bb9cf8f56e1b73cc",
            "5fb147acf7c34e71bc4be5ba320f4317",
            "2dc6b5342ba64bd68587d63592ca99bc",
            "10326f6872d1448d81f19139fe7e9865",
            "ebb32ba6fd26451c8607e05c3d3f663b",
            "e68c5b03b6e244f49daee01175d93aa1",
            "294001bb93cb45e6af759389cc3207c0",
            "f9cfb385a1e24381b99216196f323b68",
            "93e24499718d49fe9adff5f23412cc7e",
            "e3d80b10103942fb8187d4c723e25375",
            "270859a72920495db7a58b06b6d1f549",
            "029f347f67634a7891799ff0472a00e8",
            "66fb0c8e4f98401b89559c159c50b618"
          ]
        },
        "id": "Ng9Mq9ohjuKB",
        "outputId": "b867f685-8e9d-45d0-93cf-ef7d3de191a8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b70a896239cd4c33a305abe5ff52de31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Assign values:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2dc6b5342ba64bd68587d63592ca99bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Assign values:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Test passed!\n"
          ]
        }
      ],
      "source": [
        "assert my_sampler(5, [0.5, 0.5]).shape == (5,), \"Shape mismatch for 1D size\"\n",
        "assert my_sampler((2, 3), [0.1, 0.2, 0.7]).shape == (2, 3), \"Shape mismatch for 2D size\"\n",
        "print(\"✅ Test passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-SqvfpjkA23"
      },
      "source": [
        "###### Test that sampled values are within valid range"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "736c016f1ab0406f91743a6ce62ff287",
            "11c53959611344e5970ec8233094cf9e",
            "052db69fcb3f48f8a0d25bac9392f89f",
            "b5fd249b4a5b4e0a95dbfd236349cf10",
            "764961fa50844942b44e5346b6b4033a",
            "e5813a62a3924feea17955bdeb72269f",
            "de831cb2ebc244f59759d8afc2e37275",
            "c5b72d280f99483a92d9e0273e6942a1",
            "0c2d758af2ec48168ac26c954acbb22d",
            "7b167480e1e64724bd6534ada899c558",
            "5a78d9ca50c246b6b2b78a83f7292b5c"
          ]
        },
        "id": "oOtZVppmkERe",
        "outputId": "f3b5c0a7-4cec-410d-c37d-709610fee359"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "736c016f1ab0406f91743a6ce62ff287",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Assign values:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Test passed!\n"
          ]
        }
      ],
      "source": [
        "dist = [0.2, 0.3, 0.5]\n",
        "samples = my_sampler(1000, dist)\n",
        "assert samples.min().item() >= 0, \"Sampled values include values < 0\"\n",
        "assert samples.max().item() <= len(dist) - 1, \"Sampled values exceed expected maximum\"\n",
        "print(\"✅ Test passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF33DcjGkF-y"
      },
      "source": [
        "###### Test `requires_grad` flag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "66edebf9b83b42b18b5d0549a4821988",
            "c6d627076a864e95bfc13e3c20c18fba",
            "bf50e1c9577e43f9a543a0704603388e",
            "4dcb542e6d9b4406a96059214a74d190",
            "01d1469fa3e043788da46142148e1e1e",
            "5f7aae6ab6ac4584853f74423a15e3ea",
            "d3d2b3be242f41b9b79811d4738b6d4f",
            "d384c0b29a6c4ff8960187c5472b5d8a",
            "c8d8b9c27ed74dafbc09e52678b00ba3",
            "80c65658454742a78cbb142dce95c01f",
            "74f909de2b9b46929a086dbe7ac98b7a"
          ]
        },
        "id": "YdAq_qFXkGWW",
        "outputId": "40e50b18-25be-4763-bb7f-70da5dd1e6d6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66edebf9b83b42b18b5d0549a4821988",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Assign values:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Test passed!\n"
          ]
        }
      ],
      "source": [
        "t = my_sampler((4, 4), [0.1, 0.2, 0.7], requires_grad=True)\n",
        "assert t.requires_grad, \"Tensor should have requires_grad=True\"\n",
        "print(\"✅ Test passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnMA77TekPnV"
      },
      "source": [
        "###### Test that invalid distributions (not summing to 1) raise an error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6I05eK5WkN4o",
        "outputId": "43590bf1-18a8-43ee-884b-25c692235a07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Test passed!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    my_sampler(10, [0.2, 0.2, 0.2])\n",
        "    assert False, \"Expected AssertionError for invalid sum of probabilities\"\n",
        "except AssertionError as e:\n",
        "    assert \"sum to 1\" in str(e), \"Wrong error message for invalid sum\"\n",
        "print(\"✅ Test passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALkpOnvbkNe_"
      },
      "source": [
        "###### Test that negative probabilities raise an error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gTprt-kkYbM",
        "outputId": "b3adc86c-5930-4828-a927-adce9da4b8fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Test passed!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    my_sampler(10, [0.1, -0.1, 1.0])\n",
        "    assert False, \"Expected AssertionError for negative probability\"\n",
        "except AssertionError as e:\n",
        "    assert \"All probabilities must be positive\" in str(e), \"Wrong error message for negative values\"\n",
        "print(\"✅ Test passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDHYFZXbkcar"
      },
      "source": [
        "###### Test create artificial `U < p0` by using a single sample and mocked dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "86dba7a19610468bbd5b88c82ae62369",
            "5abd641fbc1a430bb62289ee69fe02e3",
            "afcc4c1568ba4b96945ec40f74c33356",
            "b15d2525622f49398b6ea519c8ed28f7",
            "efea7766728846b2918d6ec7f09693d4",
            "d5e0d1d0b0f940668993c8be194be14b",
            "63b0851c3f04405bb6fad04de68eae9b",
            "2c880d2b4c4043a9a6afd1974f319bf8",
            "1788e01180d04752ab2ef37856e21dc9",
            "844ba72fd855473d9fc034ee7b9fdf4a",
            "bc7cd111ba744479a424de386b2f811a"
          ]
        },
        "id": "JNnENmbZkeas",
        "outputId": "2fa12b71-7d1b-43d4-dfd7-970460b6e871"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86dba7a19610468bbd5b88c82ae62369",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Assign values:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Test passed!\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "dist = [0.9, 0.1]\n",
        "s = my_sampler(1000, dist)\n",
        "assert (s == 0).sum() > 800, \"Too few zeros — possible failure in U < p0 handling\"\n",
        "print(\"✅ Test passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7OBoF5BkghO"
      },
      "source": [
        "###### Test `U >= cumsum[-1]` is handled correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "f65fb9fc5eb647e3b9b1f3e619058744",
            "45e0c8f632dc4fbd856ddbd3d4a8ef23",
            "f9e1124b0c9e4caf87faad263af28b7a",
            "3caafaaf8ebb45f591eaecf770835560",
            "09a91c3975bb458db25854a5cfc201ca",
            "6031913eb25c44d4abeb010417cf1fe8",
            "a1fc31d839a746d0a110f7a8f7abc6f9",
            "9b5ca20a02d04381b450c938ac7fecd7",
            "4cfaa63fc2c241dba7a492d3fffd8083",
            "4149b96d37734439b542e112e6540505",
            "7d15ba6ed3b14acebb19435d6f0c5500"
          ]
        },
        "id": "-i04IT9nkktA",
        "outputId": "76f18f1a-1028-4d33-e4e6-18a39f597235"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f65fb9fc5eb647e3b9b1f3e619058744",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Assign values:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Test passed!\n"
          ]
        }
      ],
      "source": [
        "dist = [0.3, 0.3, 0.4]\n",
        "torch.manual_seed(123)\n",
        "s = my_sampler(10000, dist)\n",
        "assert (s == 2).sum() > 3000, \"Too few 2's — possible failure in U >= cumsum[-1] handling\"\n",
        "print(\"✅ Test passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqg9Phs5Mmjq"
      },
      "source": [
        "### Plot Distribution Histogram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gyhaVWwMsTA"
      },
      "source": [
        "After drawing 10,000 samples from the distribution `[0.1, 0.2, 0.7]`, a we plot a histogram using the allowed `matplotlib.pyplot.hist`. The bin edges are carefully chosen to align with integer values (classes 0, 1, 2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504,
          "referenced_widgets": [
            "1c7eeb0c26544085afd806717c319ec3",
            "5f64a31430fd4ed5b726e33cffaf4d38",
            "3416dc9ceb8142939439b7faaf6f100a",
            "85397c035d0143a0a6f9811d350253c5",
            "fa84cb47d1634a3286d2e469fa332955",
            "6b6cb32f41ee4cb1ae82910ae4449e98",
            "326cd33d641947d38d6bd8bc6a555943",
            "715ad09ec47246a680725292e0b7efc2",
            "f078bdf5d37c4ac08937e7c1ad672588",
            "743c761008c3446bab32d87f64e3b2d9",
            "96a6e1fbb98e4996bed81a8c0705a4d7"
          ]
        },
        "id": "mEK2tXEDBq7v",
        "outputId": "321b9f3f-d57e-44f7-ae41-f3cfc47e6795"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c7eeb0c26544085afd806717c319ec3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Assign values:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Sample 10,000 values using tqdm\n",
        "n_samples = 10000\n",
        "dist = [0.1, 0.2, 0.7]\n",
        "samples = my_sampler(n_samples, dist).numpy()\n",
        "\n",
        "# Plot histogram\n",
        "plt.hist(samples, bins=[-0.5, 0.5, 1.5, 2.5], rwidth=0.8, align='mid')\n",
        "plt.xticks([0, 1, 2])\n",
        "plt.xlabel(\"Value\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Histogram of 10,000 samples with tqdm progress\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItIFaajXM_uz"
      },
      "source": [
        "This visual representation helps confirm that the function samples values according to the specified probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYCv-b_2NNRE"
      },
      "source": [
        "## Implement Autograd System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX_63ZvwlFo5"
      },
      "source": [
        "The task is to implement a custom scalar automatic differentiation system in Python, similar to PyTorch’s autograd system for scalar values.\n",
        "\n",
        "The system should support forward computation of scalar operations such as exponentiation, logarithm, trigonometric functions, addition, multiplication, division, and power. It should also enable computing gradients via a backward pass through the computation graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kckAK6GJzU0N"
      },
      "source": [
        "### Class Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MN92vjhlk9x"
      },
      "source": [
        "The `MyScalar` class stores a value, its gradient, and a reference to the scalar(s) used to create it. A backward pass is used to compute gradients by recursively traversing the computation graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZxaaM0QlNPV7"
      },
      "outputs": [],
      "source": [
        "class MyScalar:\n",
        "    \"\"\"\n",
        "    This class represents a scalar value that supports autograd functionality.\n",
        "\n",
        "    Attributes:\n",
        "        value (float): The actual scalar value.\n",
        "        grad_wrt_parent (float): The immediate gradient with respect to the parent node.\n",
        "        parent (Optional[MyScalar]): The previous node in the computational graph.\n",
        "    \"\"\"\n",
        "    def __init__(self, value: float, grad: float = 1.0, parent: Optional['MyScalar'] = None) -> None:\n",
        "        self.value = value           # A scalar value\n",
        "        self.grad_wrt_parent = grad  # The immediate derivative at this node\n",
        "        self.parent = parent         # The previous scalar used to create this one\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        name = self.__class__.__name__\n",
        "        attributes = ', '.join(f\"{k}={v}\" for k, v in self.__dict__.items())\n",
        "        return f\"{name}({attributes})\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIfP6SdRgHmD"
      },
      "source": [
        "### Math Library Function Implementations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFmTp02EzZ5P"
      },
      "source": [
        "#### Exponential operation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sNJR5vKew8sP"
      },
      "outputs": [],
      "source": [
        "def exp(a: MyScalar) -> MyScalar:\n",
        "    \"\"\"\n",
        "    Computes e^a and tracks the gradient d/dx (e^x) = e^x.\n",
        "    \"\"\"\n",
        "    value = math.exp(a.value)\n",
        "    grad = value\n",
        "    return MyScalar(value, grad, parent=a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfo4qUBzgrfM"
      },
      "source": [
        "Sanity test for `exp`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "brQGoYW5A4km"
      },
      "outputs": [],
      "source": [
        "assert exp(MyScalar(1.0)).value == math.exp(1.0), \"Test failed\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf0phIdwze2Q"
      },
      "source": [
        "#### Logarithm operation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Kc8ZdcWhygbx"
      },
      "outputs": [],
      "source": [
        "def ln(a: MyScalar) -> MyScalar:\n",
        "    \"\"\"\n",
        "    Computes ln(a) and tracks the gradient d/dx (ln(x)) = 1/x.\n",
        "    \"\"\"\n",
        "    assert a.value > 0, \"ln is undefined for non-positive values\"\n",
        "    value = math.log(a.value)\n",
        "    grad = 1 / a.value\n",
        "    return MyScalar(value, grad, parent=a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myyKtgvXhMZD"
      },
      "source": [
        "Sanity test for `ln`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6AG8jb-VLYCL"
      },
      "outputs": [],
      "source": [
        "assert ln(MyScalar(2.0)).value == math.log(2.0), \"Test failed\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibpT15yRzef3"
      },
      "source": [
        "#### Sine operation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zF-ZxbLlyQcv"
      },
      "outputs": [],
      "source": [
        "def sin(a: MyScalar) -> MyScalar:\n",
        "    \"\"\"\n",
        "    Computes sin(a) and tracks the gradient d/dx (sin(x)) = cos(x).\n",
        "    \"\"\"\n",
        "    value = math.sin(a.value)\n",
        "    grad = math.cos(a.value)\n",
        "    return MyScalar(value, grad, parent=a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV3rubX4hXBo"
      },
      "source": [
        "Sanity test for `sin`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZoQKvMITCa1W"
      },
      "outputs": [],
      "source": [
        "assert sin(MyScalar(1.0)).value == math.sin(1.0), \"Test failed\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTMDgb4jzeQR"
      },
      "source": [
        "#### Cosine operation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TMaT6RjhyR3_"
      },
      "outputs": [],
      "source": [
        "def cos(a: MyScalar) -> MyScalar:\n",
        "    \"\"\"\n",
        "    Computes cos(a) and tracks the gradient d/dx (cos(x)) = -sin(x).\n",
        "    \"\"\"\n",
        "    value = math.cos(a.value)\n",
        "    grad = -math.sin(a.value)\n",
        "    return MyScalar(value, grad, parent=a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3zLTX6whg93"
      },
      "source": [
        "Sanity test for `cos`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fa_Ml7xxCutg"
      },
      "outputs": [],
      "source": [
        "assert cos(MyScalar(1.0)).value == math.cos(1.0), \"Test failed\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x8FwblAzdyg"
      },
      "source": [
        "#### Power operation: `a^n`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DE2EhSgdw7PB"
      },
      "outputs": [],
      "source": [
        "def power(a: MyScalar, n: float) -> MyScalar:\n",
        "    \"\"\"\n",
        "    Computes a^n and tracks the gradient d/dx (x^n) = n * x^(n - 1).\n",
        "    \"\"\"\n",
        "    value = math.pow(a.value, n)\n",
        "    grad = n * math.pow(a.value, n - 1)\n",
        "    return MyScalar(value, grad, parent=a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVoCdgafhp1t"
      },
      "source": [
        "Sanity test for `power`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6tYsUMQFC-aR"
      },
      "outputs": [],
      "source": [
        "result = power(MyScalar(2.0), 3.0)\n",
        "assert result.value == math.pow(2.0, 3.0), \"Test failed\"\n",
        "assert result.grad_wrt_parent == 3 * math.pow(2.0, 2.0), \"Test failed\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UqwJHFizdVR"
      },
      "source": [
        "#### Adding a constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oNaPS96kyTEB"
      },
      "outputs": [],
      "source": [
        "def add_const(a: MyScalar, c: float) -> MyScalar:\n",
        "    \"\"\"\n",
        "    Adds a constant to a scalar value and tracks the gradient: d/dx (x + c) = 1\n",
        "    \"\"\"\n",
        "    value = a.value + c\n",
        "    grad = 1.0\n",
        "    return MyScalar(value, grad, parent=a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znvWOyNKiG_v"
      },
      "source": [
        "Sanity test for `add_const`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1aY2mZBADXUm"
      },
      "outputs": [],
      "source": [
        "result = add_const(MyScalar(2.0), 3.0)\n",
        "assert result.value == 5.0, \"Test failed for value\"\n",
        "assert result.grad_wrt_parent == 1.0, \"Test failed for gradient\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeXUfx77zcTb"
      },
      "source": [
        "#### Multiplying by a constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jZpzQwtyyuYE"
      },
      "outputs": [],
      "source": [
        "def multiply_const(a: MyScalar, c: float) -> MyScalar:\n",
        "    \"\"\"\n",
        "    Multiplies scalar with constant and tracks the gradient d/dx (c * x) = c\n",
        "    \"\"\"\n",
        "    value = a.value * c\n",
        "    grad = c\n",
        "    return MyScalar(value, grad, parent=a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmCW8-YSiMa4"
      },
      "source": [
        "Sanity test for `multiply_const`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SK0SVpUvDlEX"
      },
      "outputs": [],
      "source": [
        "result = multiply_const(MyScalar(2.0), 3.0)\n",
        "assert result.value == 6.0, \"Test failed for value\"\n",
        "assert result.grad_wrt_parent == 3.0, \"Test failed for gradient\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZuaXnZ9zfym"
      },
      "source": [
        "### Implement `get_gradient` function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1p4YESH78sM"
      },
      "source": [
        "#### Function definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhpa0N7Mmf6S"
      },
      "source": [
        "The `get_gradient` function performs reverse automatic differentiation on a computation graph of MyScalar objects. Given a final output scalar, it recursively computes the gradient of this output with respect to all input scalars involved in its computation.\n",
        "\n",
        "Step-by-step explanation:\n",
        "\n",
        "1. The function initializes an empty dictionary to store the accumulated gradients of each node (`scalar`) in the graph.\n",
        "\n",
        "2. The function defines an inner function called `_backward` that:\n",
        " * Adds the incoming gradient (`grad`) to the current node’s total gradient.\n",
        " * Computes the gradient with respect to the parent node using the local derivative (`grad_wrt_parent`).\n",
        " * Recursively calls itself on the parent node.\n",
        "\n",
        "**Note:** The backward process starts from the final node (scaler) with path_grad = 1.0, assuming we want dy/dy = 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "5PTkcpxEy03V"
      },
      "outputs": [],
      "source": [
        "def get_gradient(scaler: MyScalar) -> Dict[MyScalar, float]:\n",
        "    \"\"\"\n",
        "    This function computes gradients of a scalar with respect to all input scalars in the computation graph.\n",
        "    It performs reverse automatic differentiation starting from the given scalar node, and traverses back through the\n",
        "    graph to accumulate gradients for each contributing node.\n",
        "\n",
        "    Args:\n",
        "        scaler (MyScalar): The scalar for which gradients are to be computed.\n",
        "\n",
        "    Returns:\n",
        "        Dict[MyScalar, float]: A dict mapping each scalar to its gradient with respect to the output.\n",
        "    \"\"\"\n",
        "    gradients = dict()\n",
        "\n",
        "    def _backward(node: MyScalar, grad: float) -> None:\n",
        "        # Accumulate the gradient from this path into the total gradient at this node\n",
        "        if node in gradients:\n",
        "            gradients[node] += grad\n",
        "        else:\n",
        "            gradients[node] = grad\n",
        "\n",
        "        # If this node has a parent, continue propagating the gradient\n",
        "        if node.parent is not None:\n",
        "            parent_grad = grad * node.grad_wrt_parent  # Apply the chain rule\n",
        "            _backward(node.parent, parent_grad)\n",
        "\n",
        "    _backward(scaler, 1.0)  # Start the backpropagation from the output node with gradient 1.0 (assuming dy/dy = 1)\n",
        "    return gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xdIgpJy79pe"
      },
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CQayzUZlvCc"
      },
      "source": [
        "To verify correctness, the gradients from the custom autograd system are compared with PyTorch’s autograd system on the same computations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEIJ-Gy9zY0I"
      },
      "source": [
        "#### Tolerance constant for floating-point comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vI9wC8bSChks"
      },
      "outputs": [],
      "source": [
        "EPSILON = 1e-5  # Tolerance for numerical comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s84NseQxjAXW"
      },
      "source": [
        "#### Compare usage example to PyTorch autograd system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvf_esshpd7J"
      },
      "source": [
        "##### Test 1: Given usage example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ioIaSBZpqfq",
        "outputId": "d0371f39-0c01-431d-c2a8-3432411b4682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MyScalar(value=2, grad_wrt_parent=1.0, parent=None)\n",
            "\n",
            "MyScalar(value=4.0, grad_wrt_parent=4.0, parent=MyScalar(value=2, grad_wrt_parent=1.0, parent=None))\n",
            "\n",
            "MyScalar(value=54.598150033144236, grad_wrt_parent=54.598150033144236, parent=MyScalar(value=4.0, grad_wrt_parent=4.0, parent=MyScalar(value=2, grad_wrt_parent=1.0, parent=None)))\n",
            "\n",
            "Gradient of MyScalar(value=54.598150033144236, grad_wrt_parent=54.598150033144236, parent=MyScalar(value=4.0, grad_wrt_parent=4.0, parent=MyScalar(value=2, grad_wrt_parent=1.0, parent=None))) with respect to c: 1.0\n",
            "Gradient of MyScalar(value=4.0, grad_wrt_parent=4.0, parent=MyScalar(value=2, grad_wrt_parent=1.0, parent=None)) with respect to c: 54.598150033144236\n",
            "Gradient of MyScalar(value=2, grad_wrt_parent=1.0, parent=None) with respect to c: 218.39260013257694\n"
          ]
        }
      ],
      "source": [
        "a = MyScalar(2)\n",
        "b = power(a, 2)\n",
        "c = exp(b)\n",
        "d = grads = get_gradient(c)\n",
        "\n",
        "for var in [a, b, c]:\n",
        "    print(var, end=\"\\n\\n\")\n",
        "\n",
        "for var, grad in grads.items():\n",
        "    print(f\"Gradient of {var} with respect to c: {grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjfRD8wGryd8"
      },
      "source": [
        "##### Perform the same operations using PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8TPQHMab7-Om"
      },
      "outputs": [],
      "source": [
        "a_torch = torch.tensor(2.0, requires_grad=True)\n",
        "b_torch = a_torch ** 2\n",
        "b_torch.retain_grad()\n",
        "c_torch = torch.exp(b_torch)\n",
        "c_torch.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzvBBiHpsA03"
      },
      "source": [
        "##### Validate the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz4NVDv6sDMN",
        "outputId": "df2cd6f6-a407-4f48-d749-c9ccfeabbd97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Test passed\n"
          ]
        }
      ],
      "source": [
        "assert abs(grads[a] - a_torch.grad.item()) < EPSILON, \"Test 1 failed: gradient of a does not match PyTorch\"\n",
        "assert abs(grads[b] - b_torch.grad.item()) < EPSILON, \"Test 1 failed: gradient of b does not match PyTorch\"\n",
        "print(\"✅ Test passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ipeWiXR9HyQ"
      },
      "source": [
        "#### Test 2: `y = exp(x^2)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZh6bsXHscoL",
        "outputId": "b81c178f-11cf-4890-9a63-525af443b39c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Test passed\n"
          ]
        }
      ],
      "source": [
        "x = MyScalar(1.0)\n",
        "y = exp(power(x, 2))\n",
        "grad = get_gradient(y)[x]\n",
        "\n",
        "x_torch = torch.tensor(1.0, requires_grad=True)\n",
        "y_torch = torch.exp(torch.pow(x_torch, 2))\n",
        "y_torch.backward()\n",
        "\n",
        "assert abs(y.value - y_torch.item()) < EPSILON, f\"Test 2 failed: y value mismatch. Expected {y_t.item()}, got {y.value}\"\n",
        "assert abs(grad - x_torch.grad.item()) < EPSILON, f\"Test 2 failed: gradient mismatch. Expected {x_t.grad.item()}, got {grad}\"\n",
        "print(\"✅ Test passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtVtytlesfSr"
      },
      "source": [
        "##### Test 3: `y = ln(x)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C47qlP-fsjQw",
        "outputId": "e16e2400-ee89-4522-f573-88af0bf842c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Test passed\n"
          ]
        }
      ],
      "source": [
        "x = MyScalar(2.0)\n",
        "y = ln(x)\n",
        "grad = get_gradient(y)[x]\n",
        "\n",
        "x_torch = torch.tensor(2.0, requires_grad=True)\n",
        "y_torch = torch.log(x_torch)\n",
        "y_torch.backward()\n",
        "\n",
        "assert abs(y.value - y_torch.item()) < EPSILON, f\"Test 3 failed: y value mismatch. Expected {y_t.item()}, got {y.value}\"\n",
        "assert abs(grad - x_torch.grad.item()) < EPSILON, f\"Test 3 failed: gradient mismatch. Expected {x_t.grad.item()}, got {grad}\"\n",
        "print(\"✅ Test passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoKzF64LslZv"
      },
      "source": [
        "##### Test 4: `y = sin(x)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjRfwSIfso6w",
        "outputId": "eac811ba-ee43-4f31-82f8-01e96482906d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Test passed\n"
          ]
        }
      ],
      "source": [
        "x = MyScalar(0.5)\n",
        "y = sin(x)\n",
        "grad = get_gradient(y)[x]\n",
        "\n",
        "x_torch = torch.tensor(0.5, requires_grad=True)\n",
        "y_torch = torch.sin(x_torch)\n",
        "y_torch.backward()\n",
        "\n",
        "assert abs(y.value - y_torch.item()) < EPSILON, f\"Test 4 failed: y value mismatch. Expected {y_t.item()}, got {y.value}\"\n",
        "assert abs(grad - x_torch.grad.item()) < EPSILON, f\"Test 4 failed: gradient mismatch. Expected {x_t.grad.item()}, got {grad}\"\n",
        "print(\"✅ Test passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOpqixhTsrfS"
      },
      "source": [
        "##### Test 5: `y = cos(x)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv0gE8-AsyRG",
        "outputId": "ad53285e-a9c0-42ae-c75f-99ea72df94e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Test passed\n"
          ]
        }
      ],
      "source": [
        "x = MyScalar(0.5)\n",
        "y = cos(x)\n",
        "grad = get_gradient(y)[x]\n",
        "\n",
        "x_torch = torch.tensor(0.5, requires_grad=True)\n",
        "y_torch = torch.cos(x_torch)\n",
        "y_torch.backward()\n",
        "\n",
        "assert abs(y.value - y_torch.item()) < EPSILON, f\"Test 5 failed: y value mismatch. Expected {y_t.item()}, got {y.value}\"\n",
        "assert abs(grad - x_torch.grad.item()) < EPSILON, f\"Test 5 failed: gradient mismatch. Expected {x_t.grad.item()}, got {grad}\"\n",
        "print(\"✅ Test passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn5etaOms9tH"
      },
      "source": [
        "##### Test 6: `y = 3 * sin(x)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c61IBn0tB52",
        "outputId": "7a0f96ee-adbf-4a96-c21f-27a9627f77f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Test passed\n"
          ]
        }
      ],
      "source": [
        "x = MyScalar(0.8)\n",
        "y = multiply_const(sin(x), 3.0)\n",
        "grad = get_gradient(y)[x]\n",
        "\n",
        "x_torch = torch.tensor(0.8, requires_grad=True)\n",
        "y_torch = 3 * torch.sin(x_torch)\n",
        "y_torch.backward()\n",
        "\n",
        "assert abs(y.value - y_torch.item()) < EPSILON, f\"Test 5 failed: y value mismatch. Expected {y_t.item()}, got {y.value}\"\n",
        "assert abs(grad - x_torch.grad.item()) < EPSILON, f\"Test 5 failed: gradient mismatch. Expected {x_t.grad.item()}, got {grad}\"\n",
        "print(\"✅ Test passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkvsVlKLtC5u"
      },
      "source": [
        "##### Test 7: `y = x^n` for float n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMkmkNIj9IQe",
        "outputId": "07f69ea6-1db5-4892-df77-4b5b0085a089"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Test passed\n"
          ]
        }
      ],
      "source": [
        "x = MyScalar(3.0)\n",
        "y = power(x, 4.0)\n",
        "grad = get_gradient(y)[x]\n",
        "\n",
        "x_torch = torch.tensor(3.0, requires_grad=True)\n",
        "y_torch = torch.pow(x_torch, 4.0)\n",
        "y_torch.backward()\n",
        "\n",
        "assert abs(y.value - y_torch.item()) < EPSILON, f\"Test 6 failed: y value mismatch. Expected {y_t.item()}, got {y.value}\"\n",
        "assert abs(grad - x_torch.grad.item()) < EPSILON, f\"Test 6 failed: gradient mismatch. Expected {x_t.grad.item()}, got {grad}\"\n",
        "print(\"✅ Test passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoW-pJKd9gXQ"
      },
      "source": [
        "## Bonus - Extend Autograd System to 2 Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66dgWnrB9l-G"
      },
      "source": [
        "### Class Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b7u5hNQu_To"
      },
      "source": [
        "In this section, we extend the automatic differentiation system described earlier to handle operations with two inputs, such as addition, multiplication, and division. To do this, we use inheritance to build upon the custom scalar class `MyScalar` and introduce a new class, `MyScalar2Inputs`, which is specifically designed to handle operations involving two parent nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xxU_B0dp9k8z"
      },
      "outputs": [],
      "source": [
        "class MyScalar2Inputs(MyScalar):\n",
        "    \"\"\"\n",
        "    This class represents a scalar value that depends on two parent scalars.\n",
        "\n",
        "    Args:\n",
        "        value (float): The resulting scalar value of the operation.\n",
        "        parent (MyScalar): The first parent node in the computation graph.\n",
        "        parent2 (MyScalar): The second parent node in the computation graph.\n",
        "        grad_wrt_parent (float): The gradient with respect to the first parent.\n",
        "        grad_wrt_parent2 (float): The gradient with respect to the second parent.\n",
        "    \"\"\"\n",
        "    def __init__(self, value: float, parent: MyScalar, parent2: MyScalar, grad_wrt_parent: float,\n",
        "                 grad_wrt_parent2: float) -> None:\n",
        "        super().__init__(value=value, grad=grad_wrt_parent, parent=parent)\n",
        "        self.parent2 = parent2\n",
        "        self.grad_wrt_parent2 = grad_wrt_parent2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y1ANm00vno4"
      },
      "source": [
        "The `__init__` function initializes these values by calling the parent class `MyScalar` constructor, and adding the `parent2` and `grad_wrt_parent2` attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g0MvXG199A5"
      },
      "source": [
        "### Define Mathematical Operations for Two Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToZ9E-4kH67-"
      },
      "source": [
        "#### Adding two `MyScalar` objects together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nczUzce2xJUE"
      },
      "source": [
        "This function computes the value as `a.value + b.value`, and returns an instance of `MyScalar2Inputs` where `parent` is `a`, `parent2` is `b.`\n",
        "\n",
        "Gradients with respect to a and b are both set to 1.0, as the derivative of a sum with respect to each variable is 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "GFQwzxHsH_ER"
      },
      "outputs": [],
      "source": [
        "def add(a: MyScalar, b: MyScalar) -> MyScalar2Inputs:\n",
        "    value = a.value + b.value\n",
        "    return MyScalar2Inputs(value, a, b, 1.0, 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4gdq89YxX81"
      },
      "source": [
        "Sanity test for `add`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "BHADKW85IP-p"
      },
      "outputs": [],
      "source": [
        "result = add(MyScalar(2.0), MyScalar(3.0))\n",
        "assert result.value == 5.0, \"Test failed: add function value mismatch\"\n",
        "assert result.grad_wrt_parent == 1.0 and result.grad_wrt_parent2 == 1.0, \"Test failed: add function gradients mismatch\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44PNf10BH_b1"
      },
      "source": [
        "#### Multiply two `MyScalar` objects together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlQ-as6WxiKZ"
      },
      "source": [
        "This function computes the value as `a.value * b.value`, and the gradients as follows:\n",
        "\n",
        "- `grad_wrt_parent = b.value` - derivative of `a * b` with respect to `a` is `b`.\n",
        "\n",
        "- `grad_wrt_parent2 = a.value` - derivative of `a * b` with respect to `b` is `a`.\n",
        "\n",
        "The function returns an instance of `MyScalar2Inputs` with the computed gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "G4Vj7MOBIAiD"
      },
      "outputs": [],
      "source": [
        "def multiply(a: MyScalar, b: MyScalar) -> MyScalar2Inputs:\n",
        "    value = a.value * b.value\n",
        "    return MyScalar2Inputs(value, a, b, b.value, a.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udZFXMm3x4Ox"
      },
      "source": [
        "Sanity test for `multiply`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "nVCXGVv1JVqf"
      },
      "outputs": [],
      "source": [
        "result = multiply(MyScalar(2.0), MyScalar(3.0))\n",
        "assert result.value == 6.0, \"Test failed: multiply function value mismatch\"\n",
        "assert result.grad_wrt_parent == 3.0, \"Test failed: multiply function gradients mismatch\"\n",
        "assert result.grad_wrt_parent2 == 2.0, \"Test failed: multiply function gradients mismatch\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dg_a_GixIBBj"
      },
      "source": [
        "#### Divide `MyScalar` by another `MyScalar`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW-GJKhAyDgq"
      },
      "source": [
        "This function divides `a` by `b`. The value is calculated as `a.value / b.value`, and the gradients are:\n",
        "\n",
        "- `grad_wrt_parent = -a.value / (b.value^2)` - derivative of `a / b` with respect to `a`.\n",
        "\n",
        "- `grad_wrt_parent2 = 1 / b.value` - derivative of `a / b` with respect to `b`).\n",
        "\n",
        "These gradients are returned as part of a `MyScalar2Inputs` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "-qBBzYNl99r5"
      },
      "outputs": [],
      "source": [
        "def divide(a: MyScalar, b: MyScalar) -> MyScalar2Inputs:\n",
        "    value = a.value / b.value\n",
        "    grad_wrt_a = 1.0 / b.value\n",
        "    grad_wrt_b = -a.value / (b.value ** 2)\n",
        "    return MyScalar2Inputs(value, a, b, grad_wrt_a, grad_wrt_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7d8e_qMyX6H"
      },
      "source": [
        "Sanity test for `divide`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "nCFmv4dMJfqC"
      },
      "outputs": [],
      "source": [
        "result = divide(MyScalar(6.0), MyScalar(3.0))\n",
        "assert result.value == 2.0, \"Test failed: divide function value mismatch\"\n",
        "assert result.grad_wrt_parent == 1.0 / 3.0, \"Test failed: divide function gradients mismatch\"\n",
        "assert result.grad_wrt_parent2 == -6.0 / 9.0, \"Test failed: divide function gradients mismatch\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF_URsVg_BOB"
      },
      "source": [
        "### Extend `get_gradient` to `MyScaler2Inputs`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evAUXSJHykNg"
      },
      "source": [
        "This function performs backpropagation to compute the gradients of all the parents of a given scaler.\n",
        "\n",
        "It uses a recursive helper inner function `_backprop` that traverses the computational graph, starting from the given `scaler`.\n",
        "\n",
        "If the current node (`scalar`) has a parent, the function propagates the gradient backward through the graph using the chain rule. If the node is a `MyScalar2Inputs` object, it computes gradients for the second parent as well and propagates it accordingly.\n",
        "\n",
        "The function accumulates the gradients in a dictionary and returns it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "JnaaD42o_BvJ"
      },
      "outputs": [],
      "source": [
        "def get_gradient(scaler: MyScalar) -> Dict[MyScalar, float]:\n",
        "    \"\"\"\n",
        "    This function computes gradients of a scalar with respect to all input scalars in the computation graph.\n",
        "    It performs reverse automatic differentiation starting from the given scalar node, and traverses back through the\n",
        "    graph to accumulate gradients for each contributing node.\n",
        "\n",
        "    Args:\n",
        "        scaler (MyScalar): The scalar for which gradients are to be computed.\n",
        "\n",
        "    Returns:\n",
        "        Dict[MyScalar, float]: A dict mapping each scalar to its gradient with respect to the output.\n",
        "    \"\"\"\n",
        "    gradients = dict()\n",
        "\n",
        "    def _backprop(node: MyScalar, grad: float) -> None:\n",
        "        if node in gradients:\n",
        "            gradients[node] += grad\n",
        "        else:\n",
        "            gradients[node] = grad\n",
        "\n",
        "        if getattr(node, 'parent', None) is not None:\n",
        "            _backprop(node.parent, grad * node.grad_wrt_parent)\n",
        "        if getattr(node, 'parent2', None) is not None:\n",
        "            _backprop(node.parent2, grad * node.grad_wrt_parent2)\n",
        "\n",
        "    _backprop(scaler, 1.0)\n",
        "    return gradients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFeb_9Q228cY"
      },
      "source": [
        "Note that this function implementation can replace the previous one, as it fully supports a `MyScalar` with a single parent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o1eLpiz_NhF"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJggSKzKzAcY"
      },
      "source": [
        "#### Custom autograd system usage example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kwADD1dzDk2",
        "outputId": "02a8819e-af6b-43ca-b9bd-435f810daa5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MyScalar(value=3.0, grad_wrt_parent=1.0, parent=None)\n",
            "\n",
            "MyScalar(value=4.0, grad_wrt_parent=1.0, parent=None)\n",
            "\n",
            "MyScalar2Inputs(value=12.0, grad_wrt_parent=4.0, parent=MyScalar(value=3.0, grad_wrt_parent=1.0, parent=None), parent2=MyScalar(value=4.0, grad_wrt_parent=1.0, parent=None), grad_wrt_parent2=3.0)\n",
            "\n",
            "MyScalar2Inputs(value=15.0, grad_wrt_parent=1.0, parent=MyScalar2Inputs(value=12.0, grad_wrt_parent=4.0, parent=MyScalar(value=3.0, grad_wrt_parent=1.0, parent=None), parent2=MyScalar(value=4.0, grad_wrt_parent=1.0, parent=None), grad_wrt_parent2=3.0), parent2=MyScalar(value=3.0, grad_wrt_parent=1.0, parent=None), grad_wrt_parent2=1.0)\n",
            "\n",
            "MyScalar2Inputs(value=4.0, grad_wrt_parent=0.3333333333333333, parent=MyScalar2Inputs(value=12.0, grad_wrt_parent=4.0, parent=MyScalar(value=3.0, grad_wrt_parent=1.0, parent=None), parent2=MyScalar(value=4.0, grad_wrt_parent=1.0, parent=None), grad_wrt_parent2=3.0), parent2=MyScalar(value=3.0, grad_wrt_parent=1.0, parent=None), grad_wrt_parent2=-1.3333333333333333)\n",
            "\n",
            "Gradient of MyScalar2Inputs(value=4.0, grad_wrt_parent=0.3333333333333333, parent=MyScalar2Inputs(value=12.0, grad_wrt_parent=4.0, parent=MyScalar(value=3.0, grad_wrt_parent=1.0, parent=None), parent2=MyScalar(value=4.0, grad_wrt_parent=1.0, parent=None), grad_wrt_parent2=3.0), parent2=MyScalar(value=3.0, grad_wrt_parent=1.0, parent=None), grad_wrt_parent2=-1.3333333333333333) with respect to d: 1.0\n",
            "Gradient of MyScalar2Inputs(value=12.0, grad_wrt_parent=4.0, parent=MyScalar(value=3.0, grad_wrt_parent=1.0, parent=None), parent2=MyScalar(value=4.0, grad_wrt_parent=1.0, parent=None), grad_wrt_parent2=3.0) with respect to d: 0.3333333333333333\n",
            "Gradient of MyScalar(value=3.0, grad_wrt_parent=1.0, parent=None) with respect to d: 0.0\n",
            "Gradient of MyScalar(value=4.0, grad_wrt_parent=1.0, parent=None) with respect to d: 1.0\n"
          ]
        }
      ],
      "source": [
        "a = MyScalar(3.0)\n",
        "b = MyScalar(4.0)\n",
        "c = multiply(a, b)\n",
        "d = add(c, a)\n",
        "e = divide(c, a)\n",
        "grads = get_gradient(e)\n",
        "\n",
        "for var in [a, b, c, d, e]:\n",
        "    print(var, end=\"\\n\\n\")\n",
        "\n",
        "for var, grad in grads.items():\n",
        "    print(f\"Gradient of {var} with respect to d: {grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFDJ38040ekq"
      },
      "source": [
        "##### Perform the same operations using PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "uwazsG-y0he2"
      },
      "outputs": [],
      "source": [
        "a_torch = torch.tensor(3.0, requires_grad=True)\n",
        "b_torch = torch.tensor(4.0, requires_grad=True)\n",
        "c_torch = a_torch * b_torch\n",
        "d_torch = c_torch + a_torch\n",
        "e_torch = c_torch / a_torch\n",
        "e_torch.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQlT7HUu0mhY"
      },
      "source": [
        "##### Validate the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4gBI8cFWvMv"
      },
      "source": [
        "Assertions for values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uApde2OWx3m",
        "outputId": "7dc0213b-cc50-43b9-d713-3f1c2fb12244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Tests passed!\n"
          ]
        }
      ],
      "source": [
        "assert c.value == c_torch.item(), f\"Expected c={c_torch.item()}, got {c.value}\"\n",
        "assert d.value == d_torch.item(), f\"Expected d={d_torch.item()}, got {d.value}\"\n",
        "assert e.value == e_torch.item(), f\"Expected e={e_torch.item()}, got {e.value}\"\n",
        "print(\"✅ Tests passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PoBZ1zKW1DC"
      },
      "source": [
        "Assertions for gradients\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDc4Oqfq_OBN",
        "outputId": "ce4f1b5c-478a-41bf-eb4c-c406ea50e831"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Tests passed!\n"
          ]
        }
      ],
      "source": [
        "assert grads[a] == a_torch.grad.item(), f\"Expected grad_a={a_torch.grad.item()}, got {grads[a]}\"\n",
        "assert grads[b] == b_torch.grad.item(), f\"Expected grad_b={b_torch.grad.item()}, got {grads[b]}\"\n",
        "print(\"✅ Tests passed!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
